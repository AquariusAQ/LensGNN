import json
import os
import torch 
import numpy as np
import random
from tqdm import tqdm

from transformers import AutoTokenizer


# MAX_TOKEN = 1024
# MAX_SAMPLE = 32768

# pubmed / arxiv 1 order
max_centor_len_txt = 2048
max_one_step_neighbor_len_txt = 256
max_two_step_neighbor_len_txt = 256
max_one_step_neighbors = 20
max_two_step_neighbors = 8
one_step_neighbors_only_title = True
two_step_neighbors_only_title = True


# pubmed 2 order
# max_centor_len_txt = 2048
# max_one_step_neighbor_len_txt = 256
# max_two_step_neighbor_len_txt = 256
# max_one_step_neighbors = 6
# max_two_step_neighbors = 2
# one_step_neighbors_only_title = True
# two_step_neighbors_only_title = True

# # cora 1 order
# max_centor_len_txt = 2048
# max_one_step_neighbor_len_txt = 256
# max_two_step_neighbor_len_txt = 256
# max_one_step_neighbors = 20
# max_two_step_neighbors = 8
# one_step_neighbors_only_title = True
# two_step_neighbors_only_title = True

# cora 2 order
# max_centor_len_txt = 2048
# max_one_step_neighbor_len_txt = 256
# max_two_step_neighbor_len_txt = 256
# max_one_step_neighbors = 12
# max_two_step_neighbors = 6
# one_step_neighbors_only_title = True
# two_step_neighbors_only_title = True

# cora 2 order
# max_centor_len_txt = 2048
# max_one_step_neighbor_len_txt = 256
# max_two_step_neighbor_len_txt = 256
# max_one_step_neighbors = 10
# max_two_step_neighbors = 5
# one_step_neighbors_only_title = True
# two_step_neighbors_only_title = True

result_root = "./llm_dataset"


def text_cuter(text, max_len_txt = 512, only_title=False):
    if only_title:
        try:
            i = text.index("Abstract")
        except:
            i = text.index("Content")
        text = text[:i]
    text = text.replace("\n", "")
    return text if len(text.split(" ")) < max_len_txt else " ".join(text.split(" ")[0:max_len_txt]) 

def list_sampler(li, max_len_list = 10):
    return li if len(li) < max_len_list else random.sample(li, k=max_len_list)

def instruction_builder(config):
    neighbor_text = ""
    if config["with_text"] == True:
        if config["with_neighbor"] == 0:
            neighbor_text = "Center node information: 'string',"
        elif config["with_neighbor"] == 1:
            neighbor_text = "first-order neighbor: ['string ',...]. Center node information: 'string',"
        elif config["with_neighbor"] == 2:
            neighbor_text = "first-order neighbor: [('string ',second-order neighbor: ['string',...]),...]. Center node information: 'string',"
        else:
            print(f"with_neighbor should be 0,1,2, using 0")
    else:
        neighbor_text = ""
    instruction = f"""\
You are a good graph model selector. \
There are representations generated by different GNN models for the same node, \
The content provided by the user each time takes the form of \
"{neighbor_text}\
[(GNN type: 'string',feature: 'vector'), ...]. \
The number of classes is 'int', which class it is?".\
Now you need to analyze these representations to provide a correct classification result. \
Note: Features are only symbols and have no direct association with the text. \
"""
    # instruction += "tag:" + "\ufffe" + config["name"] + "\ufffe"
    return instruction

def input_builder(tokenizer, config:dict, link:list, gnn_features, text:list, index:int, output_feature:int, 
                    max_centor_len_txt = 2048, 
                    max_one_step_neighbor_len_txt = 256, 
                    max_two_step_neighbor_len_txt = 256, 
                    max_one_step_neighbors = 16, 
                    max_two_step_neighbors = 8, 
                    one_step_neighbors_only_title = True, 
                    two_step_neighbors_only_title = True):
    with_gnn_feature_output = "["

    # one_hot = [0 for index in range(output_feature)]
    # one_hot[gnn_features[gnn_name]["label"][i]] = 1
    # with_gnn_feature_output += json.dumps(one_hot)

    if config["with_text"] == True:
        # neighbor node text
        if config["with_neighbor"] == 0:
            pass
        elif config["with_neighbor"] == 1:
            neighbor = []
            for j in list_sampler(link[index], max_len_list=max_one_step_neighbors):
                neighbor.append(text_cuter(text[j], max_one_step_neighbor_len_txt, one_step_neighbors_only_title))
            neighbor_text = ",".join(neighbor)
            with_gnn_feature_output += f"first-order neighbor: [{neighbor_text}]. "
        elif config["with_neighbor"] == 2:
            neighbor = []
            for j in list_sampler(link[index], max_len_list=max_one_step_neighbors):
                second_neighbor = []
                for k in list_sampler(link[j], max_len_list=max_two_step_neighbors):
                    second_neighbor.append(text_cuter(text[k], max_two_step_neighbor_len_txt, two_step_neighbors_only_title))
                neighbor.append("("+text_cuter(text[j], max_one_step_neighbor_len_txt, one_step_neighbors_only_title)+", second-order neighboring text: "+json.dumps(second_neighbor)+")")
            neighbor_text = ", ".join(neighbor)
            with_gnn_feature_output += f"first-order neighbor: [{neighbor_text}]. "
        else:
            print(f"with_neighbor should be 0,1,2, using 0")
        
        # center node text
        with_gnn_feature_output += f"Center node information: {text_cuter(text[index], max_centor_len_txt)}"

    # GNN feature
    feature_str_list = []
    # print("Token shape:",gnn_features.shape)
    for i, gnn_name in enumerate(config["with_gnn_feature"]):
        # feature_str_list.append("(GNN type: {}, feature: {})".format(gnn_name, np.array2string(gnn_features[gnn_name]["feature"][index].numpy())))
        # feature_str_list.append("(GNN type: {}, feature: {})".format(gnn_name, tokenizer.convert_tokens_to_string(gnn_features[i][index].cpu().numpy().tolist())))
        feature_str_list.append("(GNN type: {}, feature: {})".format(gnn_name, "".join(["\uffff"] * config["graph_token_shape"][0])))
    with_gnn_feature_output += ",".join(feature_str_list)
    with_gnn_feature_output += "]. "

    # num class
    with_gnn_feature_output += f"The number of classes is {output_feature}, which class it is?"

    with_gnn_feature_output += "\ufffe" + config["name"] + "\ufffe"

    return with_gnn_feature_output




def build_llm_dataset(gnn_features, config, varbose=True):
    model_id = "./LLM/Baichuan2-13B-Chat"
    tokenizer = AutoTokenizer.from_pretrained(model_id,
        revision="v2.0",
        use_fast=False,
        trust_remote_code=True)
    
    output_feature = -1
    MAX_TOKEN=config["max_token"]

    ##############################
    # read graph struct and text #
    ##############################
    if config["use_dataset"] == "pubmed":
        from GNN.dataset_utils.load_pubmed import get_raw_text_pubmed as get_raw_text
        output_feature = 3
    elif config["use_dataset"] == "cora":
        from GNN.dataset_utils.load_cora import get_raw_text_cora as get_raw_text
        output_feature = 7
    elif config["use_dataset"] == "ogbn_arxiv":
        from GNN.dataset_utils.load_arxiv import get_raw_text_arxiv as get_raw_text
        output_feature = 40
    elif config["use_dataset"] == "ogbn_products":
        from GNN.dataset_utils.load_products import get_raw_text_products as get_raw_text
        output_feature = 40
    elif config["use_dataset"] == "wikics":
        from GNN.dataset_utils.load_wikics import get_raw_text_wikics as get_raw_text
        output_feature = 40
    else:
        print("dataset {} not found".format(config["use_dataset"]))
    dataset_raw, text = get_raw_text(root="./GNN", use_text=True)

    edge = dataset_raw.edge_index
    print("Reversing...")
    edge_reverse = torch.stack([edge[1], edge[0]], dim=0)
    print("Catting...")
    dataset_raw.edge_index = torch.cat([edge, edge_reverse], dim=1)
    print("Uniquing...")
    dataset_raw.edge_index = torch.unique(dataset_raw.edge_index, dim=1)
    print("Okay")

    if varbose:
        print("Graph data: ", dataset_raw)


    ###################
    # build link list #
    ###################
    link = [[] for i in range(len(text))]
    edge_index = dataset_raw["edge_index"].numpy()
    for i in range(edge_index.shape[1]):
        link[edge_index[0][i]].append(edge_index[1][i])
    # print(link)
    
    model_id = "./LLM/Baichuan2-13B-Chat"
    tokenizer = AutoTokenizer.from_pretrained(model_id,
        revision="v2.0",
        use_fast=False,
        trust_remote_code=True)

    max_token_count = 0
    # dataset = []

    #################
    # split dataset #
    #################
    # print("Data size: ", len(dataset))
    # train_data_size = int(len(dataset) * 0.8)
    # test_data_size = len(dataset) - train_data_size
    # print("Train size: ", train_data_size, " test size: ", test_data_size)

    # from torch_geometric.utils import train_test_split_edges, negative_sampling

    # data = train_test_split_edges(data, val_ratio=0.2, test_ratio=0.2)


    train_id = dataset_raw["train_id"] # np.concatenate((data["train_id"], data["val_id"]))
    val_id = dataset_raw["val_id"]
    test_id = dataset_raw["test_id"]

    # train_size = 4
    # val_size = 0
    # test_size = 0
    # train_size = 0
    train_size = len(train_id)
    val_size = len(val_id)
    test_size = min(len(test_id), config["max_test_sample"])
    print("Train size: ", train_size, "val size",  val_size, "test size: ", test_size)


    train_data = []
    val_data = []
    test_data = []


    data_itr_start = 0
    data_itr_end = len(text)

    skip_list = []


    from utils import hash_list
    new_vector_dataset = {}
    new_dataset_origin_text = {}

    if len(config["with_gnn_feature"]) > 0:
        print(gnn_features.shape)
        gnn_features = gnn_features.reshape((len(config["with_gnn_feature"]), -1, config["graph_token_shape"][0], config["embedding_size_pre_token"]))

    
    for idx, i in tqdm(enumerate(train_id), desc="Processing train data", total=len(train_id)): 
        
        data = {
                "instruction": instruction_builder(config),
                "input": input_builder(tokenizer=tokenizer, config=config, link=link, gnn_features=gnn_features, text=text, index=i, output_feature=output_feature, 
                    max_centor_len_txt = max_centor_len_txt, 
                    max_one_step_neighbor_len_txt = max_one_step_neighbor_len_txt, 
                    max_two_step_neighbor_len_txt = max_two_step_neighbor_len_txt, 
                    max_one_step_neighbors = max_one_step_neighbors, 
                    max_two_step_neighbors = max_two_step_neighbors, 
                    one_step_neighbors_only_title = True, 
                    two_step_neighbors_only_title = True),
                "output": str(dataset_raw["y"][i].numpy().item()),
                "system": "Only output results",
                "history": []
            }

        cut_down = 0
        while True:
            if cut_down >= max_one_step_neighbors:
                print("Skiping too long text..…")
                break
            token_full = "instruction:{} input:{} output:{} system:{}"\
                .format(data["instruction"], data["input"], data["output"], data["system"])
            token_size = len(tokenizer(token_full)['input_ids'])
            if token_size <= MAX_TOKEN - 3:
                if token_size > max_token_count:
                    max_token_count = token_size
                break
            else:
                # print(f"Warning: Data {i} exceed MAX_TOKEN {MAX_TOKEN}: {token_size}")
                cut_down += 1
                # print(f"Cutting down max neighbors by {cut_down}")
                data["input"] = input_builder(tokenizer=tokenizer, config=config, link=link, gnn_features=gnn_features, text=text, index=i, output_feature=output_feature, 
                    max_centor_len_txt = max_centor_len_txt - cut_down*128,#1792, 
                    max_one_step_neighbor_len_txt = max_one_step_neighbors, 
                    max_two_step_neighbor_len_txt = max_two_step_neighbors, 
                    max_one_step_neighbors = max(0, max_one_step_neighbors-cut_down), 
                    max_two_step_neighbors = max(0, max_two_step_neighbors-cut_down), 
                    # max_one_step_neighbors = max(0, max_one_step_neighbors-cut_down), 
                    # max_two_step_neighbors = max(0, max_two_step_neighbors-cut_down), 
                    one_step_neighbors_only_title = True, 
                    two_step_neighbors_only_title = True)
        
        # if type(data["input"]) != str:
        #     print(data["input"])
        
        if len(config["with_gnn_feature"]) > 0:
            inp = data["instruction"] + "\n" + data["input"]
            input_ids = tokenizer(inp)["input_ids"]
            ha = hash_list(input_ids)
            new_vector_dataset[ha] = gnn_features[:,i].detach()#.numpy().tolist()
            new_dataset_origin_text[ha] = inp
        # 14417617
        # 9440084
        # 14886727
        # 11359094

        skip_list.append(cut_down)
        train_data.append(data)

        # break

    first_objects_by_output = {}

    if len(config["with_gnn_feature"]) == 1:
        gnn = config["with_gnn_feature"][0]
    else:
        gnn = "_".join(config["with_gnn_feature"]) + "_essembled"
    if config["use_dataset"] == "ogbn_arxiv":
        target_layer = 4
    else:
        target_layer = 8

    use_dataset = config["use_dataset"]

    for idx, i in tqdm(enumerate(test_id), desc="Processing test data", total=len(test_id)):
        
        
        if idx >= test_size:
            break
        data = {
                "instruction": instruction_builder(config),
                "input": input_builder(tokenizer=tokenizer, config=config, link=link, gnn_features=gnn_features, text=text, index=i, output_feature=output_feature, 
                    max_centor_len_txt = max_centor_len_txt, 
                    max_one_step_neighbor_len_txt = max_one_step_neighbor_len_txt, 
                    max_two_step_neighbor_len_txt = max_two_step_neighbor_len_txt, 
                    max_one_step_neighbors = max_one_step_neighbors, 
                    max_two_step_neighbors = max_two_step_neighbors, 
                    one_step_neighbors_only_title = True, 
                    two_step_neighbors_only_title = True),
                "output": str(dataset_raw["y"][i].numpy().item()),
                "system": "Only output results",
                "history": []
            }


        cut_down = 0
        while True:
            if cut_down >= max_one_step_neighbors:
                print("Skiping too long text..…")
                break
            token_full = "instruction:{} input:{} output:{} system:{}"\
                .format(data["instruction"], data["input"], data["output"], data["system"])
            token_size = len(tokenizer(token_full)['input_ids'])
            if token_size <= MAX_TOKEN - 3:
                if token_size > max_token_count:
                    max_token_count = token_size
                break
            else:
                # print(f"Warning: Data {i} exceed MAX_TOKEN {MAX_TOKEN}: {token_size}")
                cut_down += 1
                # print(f"Cutting down max neighbors by {cut_down}")
                data["input"] = input_builder(tokenizer=tokenizer, config=config, link=link, gnn_features=gnn_features, text=text, index=i, output_feature=output_feature, 
                    max_centor_len_txt = max_centor_len_txt - cut_down*128,#1792, 
                    max_one_step_neighbor_len_txt = max_one_step_neighbors, 
                    max_two_step_neighbor_len_txt = max_two_step_neighbors, 
                    max_one_step_neighbors = max(0, max_one_step_neighbors-cut_down), 
                    max_two_step_neighbors = max(0, max_two_step_neighbors-cut_down), 
                    # max_one_step_neighbors = max(0, max_one_step_neighbors-cut_down), 
                    # max_two_step_neighbors = max(0, max_two_step_neighbors-cut_down), 
                    one_step_neighbors_only_title = True, 
                    two_step_neighbors_only_title = True)
        
        # if type(data["input"]) != str:
        #     print(data["input"])

        if len(config["with_gnn_feature"]) > 0:
            inp = data["instruction"] + "\n" + data["input"]
            input_ids = tokenizer(inp)["input_ids"]
            ha = hash_list(input_ids)
            new_vector_dataset[ha] = gnn_features[:,i].detach()#.numpy().tolist()
            new_dataset_origin_text[ha] = inp

        skip_list.append(cut_down)
        test_data.append(data)

    from utils import save_to_vector_dataset
    save_to_vector_dataset(new_vector_dataset, "./feature_cache/{}.pt".format(config["name"]))

    print("Longest token length:", max_token_count)
    # print("Skip", len(skip_list), "too long data.")
    skip_list = sorted(skip_list)
    from collections import Counter
    counts = Counter(skip_list)
    print("Cut down distribution:")
    print(counts)
    print("Data sample:")
    print(train_data[0])
    print("Train size: ", train_size, "val size",  val_size, "test size: ", test_size)
    print("Data size: train {}, test {}".format(len(train_data), len(test_data)))
    if train_size != len(train_data) or test_size != len(test_data):
        print("Dataset process wrong!!!!!")
        # exit(0)

    
    label = dataset_raw["y"]
    train_label = np.array([label[i] for i in train_id], dtype=np.int8)[0:train_size]
    # val_label = np.array([label[i] for i in val_id], dtype=np.int8)
    test_label = np.array([label[i] for i in test_id], dtype=np.int8)[0:test_size]
    if len(test_label) > config["max_test_sample"]:
        test_label = test_label[0:config["max_test_sample"]]
    print("Avg label")
    print(train_label.sum()/train_label.shape[0], test_label.sum()/test_label.shape[0])
    print(train_label[0:200].sum()/200, test_label[-200:].sum()/200)


    train_dataset_path = os.path.join(result_root, "gnn_{}_train.json".format(config["name"]))
    test_dataset_path = os.path.join(result_root, "gnn_{}_test.json".format(config["name"]))
    train_label_path = os.path.join(result_root, "gnn_{}_train_label.json".format(config["name"]))
    test_label_path = os.path.join(result_root, "gnn_{}_test_label.json".format(config["name"]))


    print("Saving dataset...")
    if train_size > 0:
        with open(train_dataset_path, "w") as f:
            json.dump(train_data, f, indent=4)
        with open(train_label_path, "w") as f:
            json.dump(train_label.tolist(), f, indent=4)
    with open(test_dataset_path, "w") as f:
        json.dump(test_data, f, indent=4)
    with open(test_label_path, "w") as f:
        json.dump(test_label.tolist(), f, indent=4)

    print("LLM dataset saved to: ", train_dataset_path, test_dataset_path, train_label_path, test_label_path)
    return train_dataset_path, test_dataset_path, train_label_path, test_label_path
